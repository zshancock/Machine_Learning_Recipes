{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data preprocessing - Working with Text\n",
    "\n",
    "Zac Hancock (zshancock@gmail.com)\n",
    "\n",
    "'''\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# several commonly used vectorizer setting - stop words set to english for all. \n",
    "\n",
    "#  unigram boolean vectorizer, set minimum document frequency to 5\n",
    "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram term frequency vectorizer, set minimum document frequency to 5\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram and bigram term frequency vectorizer, set minimum document frequency to 5\n",
    "gram12_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram tfidf vectorizer, set minimum document frequency to 5\n",
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=5, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With previously vectorization selection - transform into a dataframe\n",
    "\n",
    "# for this example - I used the default CountVectorizer()\n",
    "\n",
    "input_corpus = 'This is where the data is read, this should be in list format, CountVectorizer has a built in tokenizer.'\n",
    "vectorizer = CountVectorizer(stop_words = 'english') # replace with above variants if desired.\n",
    " \n",
    "\n",
    "# fit transform does both steps in one, alternatively each can be called individually (.fit(), .transform())   \n",
    "vectorizer_results = vectorizer.fit_transform()\n",
    "columns = vectorizer_results.get_feature_names()\n",
    "vectorized_df = pd.DataFrame(vectorizer_results.toarray(), columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize preparation by splitting into X_train, etc...\n",
    "\n",
    "# Labels in the split would be if this data was examining Reviews and the label was sentiment, or answers on \n",
    "# a test and the labels were right/wrong, etc.\n",
    "\n",
    "# split the data into 80 - 20%, random seed set to 0. (can be any number, just be consistent)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_df, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Examine shape. \n",
    "print('Shape of Training ' ,  X_train.shape)\n",
    "print('Shape of Testing ' , X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is ready for analysis by any number of sklearn algorithms (MultinomialNB, BernoulliNB, etc.) \n",
    "# Be sure that your vectorizer is appropriate (i.e. BernoulliNB expects binary = True, whereas Multinomial expects frequencies)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
